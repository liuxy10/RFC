{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from khrylib.utils import *\n",
    "from khrylib.rl.core.policy_gaussian import PolicyGaussian\n",
    "from khrylib.rl.core.critic import Value\n",
    "from khrylib.rl.agents import AgentPPO\n",
    "from khrylib.models.mlp import MLP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from motion_imitation.envs.humanoid_im import HumanoidEnv\n",
    "from motion_imitation.envs.humanoid_ia import HumanoidImpAwareEnv\n",
    "from motion_imitation.utils.config import Config\n",
    "from motion_imitation.reward_function import reward_func\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    cfg = \"0202\"\n",
    "    render = False\n",
    "    test = False\n",
    "    num_threads = 1\n",
    "    max_iter_num = 100\n",
    "    gpu_index = 0\n",
    "    iter = 0\n",
    "    show_noise = False\n",
    "    save_model_interval = 1\n",
    "    imp_aware = False\n",
    "\n",
    "args = Args()\n",
    "if args.render:\n",
    "    args.num_threads = 1\n",
    "cfg = Config(args.cfg, args.test, create_dirs=not (args.render or args.iter > 0))\n",
    "if args.imp_aware:\n",
    "    change_config_path_via_args(cfg, args.cfg, '_ia')\n",
    "if args.save_model_interval is not None:\n",
    "    cfg.save_model_interval = int(args.save_model_interval)\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "device = torch.device('cuda', index=args.gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(args.gpu_index)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "tb_logger = SummaryWriter(cfg.tb_dir) if not args.render else None\n",
    "logger = create_logger(os.path.join(cfg.log_dir, 'log.txt'), file_handle=not args.render)\n",
    "\n",
    "\"\"\"environment\"\"\"\n",
    "env = HumanoidImpAwareEnv(cfg) if args.imp_aware else HumanoidEnv(cfg)\n",
    "env.seed(cfg.seed)\n",
    "actuators = env.model.actuator_names\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "running_state = ZFilter((state_dim,), clip=5)\n",
    "\n",
    "\"\"\"define actor and critic\"\"\"\n",
    "policy_net = PolicyGaussian(MLP(state_dim, cfg.policy_hsize, cfg.policy_htype), action_dim, log_std=cfg.log_std, fix_std=cfg.fix_std)\n",
    "value_net = Value(MLP(state_dim, cfg.value_hsize, cfg.value_htype))\n",
    "if args.iter > 0:\n",
    "    cp_path = '%s/iter_%04d.p' % (cfg.model_dir, args.iter)\n",
    "    logger.info('loading model from checkpoint: %s' % cp_path)\n",
    "    model_cp = pickle.load(open(cp_path, \"rb\"))\n",
    "    policy_net.load_state_dict(model_cp['policy_dict'])\n",
    "    value_net.load_state_dict(model_cp['value_dict'])\n",
    "    running_state = model_cp['running_state']\n",
    "to_device(device, policy_net, value_net)\n",
    "\n",
    "\n",
    "if cfg.policy_optimizer == 'Adam':\n",
    "    optimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=cfg.policy_lr, weight_decay=cfg.policy_weightdecay)\n",
    "else:\n",
    "    optimizer_policy = torch.optim.SGD(policy_net.parameters(), lr=cfg.policy_lr, momentum=cfg.policy_momentum, weight_decay=cfg.policy_weightdecay)\n",
    "if cfg.value_optimizer == 'Adam':\n",
    "    optimizer_value = torch.optim.Adam(value_net.parameters(), lr=cfg.value_lr, weight_decay=cfg.value_weightdecay)\n",
    "else:\n",
    "    optimizer_value = torch.optim.SGD(value_net.parameters(), lr=cfg.value_lr, momentum=cfg.value_momentum, weight_decay=cfg.value_weightdecay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0003\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_policy\n",
    "policy_net, value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward functions\n",
    "expert_reward = reward_func[cfg.reward_id]\n",
    "\n",
    "\"\"\"create agent\"\"\"\n",
    "agent = AgentPPO(env=env, dtype=dtype, device=device, running_state=running_state,\n",
    "                 custom_reward=expert_reward, mean_action=args.render and not args.show_noise,\n",
    "                 render=args.render, num_threads=args.num_threads,\n",
    "                 policy_net=policy_net, value_net=value_net,\n",
    "                 optimizer_policy=optimizer_policy, optimizer_value=optimizer_value, opt_num_epochs=cfg.num_optim_epoch,\n",
    "                 gamma=cfg.gamma, tau=cfg.tau, clip_epsilon=cfg.clip_epsilon,\n",
    "                 policy_grad_clip=[(policy_net.parameters(), 40)], end_reward=cfg.end_reward,\n",
    "                 use_mini_batch=cfg.mini_batch_size < cfg.min_batch_size, mini_batch_size=cfg.mini_batch_size)\n",
    "\n",
    "\n",
    "def pre_iter_update(i_iter):\n",
    "    cfg.update_adaptive_params(i_iter)\n",
    "    agent.set_noise_rate(cfg.adp_noise_rate)\n",
    "    set_optimizer_lr(optimizer_policy, cfg.adp_policy_lr)\n",
    "    if cfg.fix_std:\n",
    "        policy_net.action_log_std.fill_(cfg.adp_log_std)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<khrylib.rl.core.trajbatch.TrajBatch at 0x7f6ab9279d60>,\n",
       " <khrylib.rl.core.logger_rl.LoggerRL at 0x7f6ab9279820>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_iter_update(args.iter)\n",
    "agent.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i_iter in tqdm(range(args.iter, args.max_iter_num)):\n",
    "i_iter = 0\n",
    "\n",
    "\"\"\"generate multiple trajectories that reach the minimum batch_size\"\"\"\n",
    "pre_iter_update(i_iter)\n",
    "batch, log = agent.sample(cfg.min_batch_size)\n",
    "if cfg.end_reward:\n",
    "    agent.env.end_reward = log.avg_c_reward * cfg.gamma / (1 - cfg.gamma)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"update networks\"\"\"\n",
    "t0 = time.time()\n",
    "agent.update_params(batch)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50015, 38), (50015,), (50015,), (50015, 76), (50015,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.actions.shape, batch.rewards.shape, batch.masks.shape, batch.states.shape,batch.exps.shape\n",
    "# batch.rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
